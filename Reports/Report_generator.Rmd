---
title: "Analysis music on Spotify and predictions of preferences"
author: "Karol Minuth"
date: "3 06 2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

#### Data comes from https://www.kaggle.com/geomack/spotifyclassification

### The purpose of this project is analyzing data, checking influence features of music on target, which is representing that song is set as not pleasant or pleasant for user

```{r include=FALSE}
# Inlcuding modules
library(pROC)
library(caTools)
library(caret)
library(rpart)
library(rpart.plot)
library(data.table)
library(varhandle)
library(ggplot2)
```

```{r echo=FALSE}
rm(list=ls())
df <- read.csv("C:/Users/Karol/R_Projects/WIZ/PROJECT/data.csv", header=TRUE, sep=",")
df$target_factor <- factor(df$target)
df$duration_in_s <- (df$duration_ms)/1000
t <- 0.5
TRAIN_DATA_SIZE = 0.8
CONSTANT_SEED <- 11
```

```{r echo=FALSE}
# for data without normalization
d <- copy(df)
n <- nrow(d)
i <- floor(TRAIN_DATA_SIZE * n)
set.seed(CONSTANT_SEED)
s <- sample.int(n, i, replace = F)
d.train <- d[s,]
d.test <- d[-s,]
```

```{r echo=FALSE}
# for data with normalization
split <- sample.split(df$target, SplitRatio=TRAIN_DATA_SIZE)
df_train <- subset(df, split == TRUE)
df_test <- subset(df, split == FALSE)

df_train[, !names(df_train) %in% c("target", "mode", "artist", "song_title", "target_factor")] <- scale(df_train[, !names(df_train) %in% c("target", "mode", "artist", "song_title", "target_factor")])

df_test[, !names(df_test) %in% c("target", "mode", "artist", "song_title", "target_factor")] <- scale(df_test[, !names(df_test) %in% c("target", "mode", "artist", "song_title", "target_factor")])
```

### Data analysis, presentation and visualization

#### Target presentation

```{r echo=FALSE}
plot(df$target_factor, main="Likes or not likes music")
```

```{r echo=FALSE}
not_likes <- df[df$target_factor == "0",]
likes <- df[df$target_factor == "1",]
not_likes_amonut <- as.numeric(length(not_likes$target_factor))
likes_amount <- as.numeric(length(likes$target_factor))
data_pie <- c(not_likes_amonut, likes_amount)
lbls <- paste(names(data_pie))
pct <- round(data_pie/sum(data_pie)*100)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
colors <- c("red", "green")
pie(data_pie,labels = lbls, col=colors, main="Likes or not likes music")
label_no_chunk <- paste('Not Likes -> ', as.character(data_pie[1]))
label_chunk <- paste('Likes -> ', as.character(data_pie[2]))
lbls <- c(label_no_chunk, label_chunk)
legend(.9, .1, lbls, cex = 0.7, fill = colors)
```

##### One of the advantage of this dataset is almost equal amount of negative and positive values of target

<br/><br/>

```{r echo=FALSE}
hist(df$tempo, breaks = 15, probability = T, xlab = "Tempo", main = "Distribution of tempo", col = "blue")
lines(density(df$tempo), col="red", lwd=3, lty=5)
```

##### The most favourite music are songs with tempo in range 125+-5

<br/><br/>

```{r echo=FALSE}
hist(df$tempo, breaks = 25, probability = T, xlab = "Tempo",  main = "Distribution of tempo", col = "blue")
lines(density(df$tempo), col="red", lwd=3, lty=5)
```

##### Here is shown that smaller range of tempo will be from 125 to 130

<br/><br/>

```{r echo=FALSE}
hist(df$duration_in_s, breaks=30, probability = T, xlab = "Duration in seconds",  main = "Distribution of duration time of songs", col = 'yellow')
lines(density(df$duration_in_s), col="red", lwd=3, lty=5)
```

##### In above chart is shown the advantage of duration in rnage from 200 to 250 seconds

<br/><br/>

```{r echo=FALSE}
hist(df$duration_in_s, breaks=40, probability = T, xlab = "Duration in seconds",  main = "Distribution of duration time of songs", col = 'yellow')
lines(density(df$duration_in_s), col="red", lwd=3, lty=5)
```

##### Here is shown that the most of songs have 220 +- 20 seconds

<br/><br/><br/>

#### In below diagrams are shown the influences of the most relevant variables for target with easily distinct differences 

```{r echo=FALSE}
plot(df$duration_in_s~df$target_factor, xlab = 'Target', ylab = 'Duration in seconds')
```

##### In above chart it is possible to notice little bigger mean values and median value for duration in seconds for target equal one

<br/><br/>

```{r echo=FALSE}
plot(df$acousticness~df$target_factor, xlab = 'Target', ylab = 'Acousticness')
```

##### In above chart it is possible to notice little bigger mean values and median value for acousticness for target equal zero. 
##### According of that it is likely to notice that user not enjoy with large values of acousticness

<br/><br/>

```{r echo=FALSE}
plot(df$energy~df$target_factor, xlab = 'Target', ylab = 'Energy')
```

##### In above chart it is possible to notice little bigger mean values and median value for energy for target equal zero. 
##### According of that it is likely to notice that user not enjoy with large values of energy

<br/><br/>

```{r echo=FALSE}
plot(df$tempo~df$target_factor, xlab = 'Target', ylab = 'Tempo')
```

##### In above chart it is possible to notice little bigger mean values and median value for energy for target equal zero. 
##### According of that it is likely to notice that user not enjoy with large values of energy

<br/><br/>

```{r echo=FALSE}
plot(df$instrumentalness~df$target_factor, xlab = 'Target', ylab = 'Instrumentalness')
```

##### In above chart it is possible to notice much bigger mean values for instrumentalness for target equal one. 
##### According of that it is likely to notice that user really enjoy of instrumentalness

<br/><br/>

```{r echo=FALSE}
plot(df$danceability~df$target_factor, xlab = 'Target', ylab = 'Danceability')
```

##### In above chart it is possible to notice much bigger mean values and median value for danceability for target equal one. ##### According of that it is likely to notice that user really enjoy with danceability songs

<br/> <br/><br/>

#### In below diagram is shown the influence between 2 variables

```{r echo=FALSE}
plot(df$danceability~df$tempo, pch=2, xlab ='Tempo', ylab = 'Danceability')
```

##### In above chart it is possible to notice that the best songs to dance are with tempo equal more or less 130

<br/><br/><br/>

### Classification and predictions of targets

```{r echo=FALSE}
model_lr_nor <- glm(data = df_train, target ~ acousticness + danceability + duration_ms + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo, family = 'binomial')

pred_proba_lr_nor <- predict(model_lr_nor, newdata = df_test, type = 'response')

hist(pred_proba_lr_nor, xlab = "Values of probabilites",  main = "Distribution of probabilites for LR with normalization", col = "brown")

df_test$predict_lr_nor <- 0
df_test[pred_proba_lr_nor>t, ]$predict_lr_nor <- 1

precision <- posPredValue(factor(df_test$predict_lr_nor, levels=c(1,0)), factor(df_test$target, levels=c(1,0)),
                          positive="1")
recall <- sensitivity(factor(df_test$predict_lr_nor, levels=c(1,0)), factor(df_test$target, levels=c(1,0)), positive="1")

F1_lr_nor <- (2 * precision * recall) / (precision + recall)
```

```{r echo=FALSE}
model_lr <- glm(data = d.train, target ~ acousticness + danceability + duration_in_s + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo, family = 'binomial')

pred_proba_lr <- predict(model_lr, newdata = d.test, type = 'response')

hist(pred_proba_lr, xlab = "Values of probabilites",  main = "Distribution of probabilites for LR without normalization", col = "brown")

d.test$predict_lr <- 0
d.test[pred_proba_lr>t, ]$predict_lr <- 1

precision <- posPredValue(factor(d.test$predict_lr, levels=c(1,0)), factor(d.test$target, levels=c(1,0)), positive="1")
recall <- sensitivity(factor(d.test$predict_lr, levels=c(1,0)), factor(d.test$target, levels=c(1,0)), positive="1")

F1_lr <- (2 * precision * recall) / (precision + recall)
```

```{r include=FALSE}
roc.model_lr_nor <- roc(df_test$target, pred_proba_lr_nor)
roc.model_lr <- roc(d.test$target, pred_proba_lr)
```

```{r echo=FALSE}
colors <- c("red", "blue")
plot(roc.model_lr_nor, col=colors[1], main="ROC for Logistic Regression")
lines(roc.model_lr, col=colors[2])

label_lr_nor <- paste('With normalization')
label_lr <- paste('Without normalization')
lbls <- c(label_lr_nor, label_lr)
legend(0.2, 0.3, lbls, cex = 0.7, fill = colors)
```

```{r echo=FALSE, message = TRUE}
cat('F1 score for Logistic Regression with normalization -> ', F1_lr_nor)
cat('F1 score for Logistic Regression without normalization -> ', F1_lr)
```

<br/><br/>

#### Trees with normalization

```{r echo=FALSE}
model_trees_nor <- rpart(data=df_train, target ~ acousticness + danceability + duration_in_s + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo)

prp(model_trees_nor)

pred_proba_trees_nor <- predict(model_trees_nor, newdata = df_test, type = 'vector')

hist(pred_proba_trees_nor, xlab = "Values of probabilites",  main = "Distribution of probabilites for Trees with normalization", col = "orange")

df_test$predict_trees_nor <- 0
df_test[pred_proba_trees_nor>t, ]$predict_trees_nor <- 1

precision <- posPredValue(factor(df_test$predict_trees_nor, levels=c(1,0)), factor(df_test$target, levels=c(1,0)), 
                          positive="1")
recall <- sensitivity(factor(df_test$predict_trees_nor, levels=c(1,0)), factor(df_test$target, levels=c(1,0)), 
                      positive="1")

F1_trees_nor <- (2 * precision * recall) / (precision + recall)
```

<br/><br/>

#### Trees without normalization

```{r echo=FALSE}
model_trees <- rpart(data=d.train,  target ~ acousticness + danceability + duration_in_s + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo)

prp(model_trees)

pred_proba_trees <- predict(model_trees, newdata = d.test, type = 'vector')

hist(pred_proba_trees, xlab = "Values of probabilites",  main = "Distribution of probabilites for Trees without normalization", col = "orange")

d.test$predict_trees <- 0
d.test[pred_proba_trees>t, ]$predict_trees <- 1

precision <- posPredValue(factor(d.test$predict_trees, levels=c(1,0)), factor(d.test$target, levels=c(1,0)), positive="1")
recall <- sensitivity(factor(d.test$predict_trees, levels=c(1,0)), factor(d.test$target, levels=c(1,0)), positive="1")

F1_trees <- (2 * precision * recall) / (precision + recall)
```

```{r include=FALSE}
roc.model_trees_nor <- roc(df_test$target, pred_proba_trees_nor)
roc.model_trees <- roc(d.test$target, pred_proba_trees)
```

```{r echo=FALSE}
colors <- c("purple", "pink")
plot(roc.model_trees_nor, col=colors[1], main="ROC for Trees")
lines(roc.model_trees, col=colors[2])

label_trees_nor <- paste('With normalization')
label_trees <- paste('Without normalization')
lbls <- c(label_trees_nor, label_trees)
legend(0.2, 0.3, lbls, cex = 0.7, fill = colors)
```

```{r echo=FALSE, message=FALSE}
cat('F1 score for Trees with normalization -> ', F1_trees_nor)
cat('F1 score for Trees without normalization -> ', F1_trees)
```


```{r echo=FALSE}
colors <- c("orange", "blue")
plot(roc.model_trees, col=colors[1], main="ROC Trees vs LR - both without normalization")
lines(roc.model_lr, col=colors[2])

label_trees <- paste('Trees')
label_lr <- paste('Logistic Regression')
lbls <- c(label_trees, label_lr)
legend(0.2, 0.3, lbls, cex = 0.7, fill = colors)
```

<br/><br/>

### Classification and predictions with hyperparameters tunning
##### It might be done with random tuning of hyperparameters (random values and searching hiperparameters spaces, but for visualizing purposes it is done with Cross-validation and manual(Grid Search) of hyperparameters - it is using a sequence of specific parameters with proper range. It might be called 'hyperparameters space')

<br/><br/>

#### Support Vector Machine - Support Vector Classifier (SVM - SVC)
##### It is used with 'polynomial' kernel

```{r echo=FALSE}
man_grid <- expand.grid(degree = seq(from = 1, to = 7, by = 1),
                        C = seq(from = 1, to = 22, by = 2),
                        scale = seq(from = 0.01, to = 0.1, by = 0.03))

fitControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 5,
                           search = "grid")

model_svc <- train(form = factor(target) ~ acousticness + danceability + duration_in_s + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo, 
                   data = d.train,
                   method = "svmPoly", 
                   trControl = fitControl,
                   verbose = FALSE,
                   tuneGrid = man_grid)

plot(model_svc)
plot(model_svc, metric = "Accuracy", plotType = "level")

pred_target_svc <- predict(model_svc, newdata = d.test)

precision <- posPredValue(pred_target_svc, factor(d.test$target, levels=c(1,0)), positive="1")
recall <- sensitivity(pred_target_svc, factor(d.test$target, levels=c(1,0)),  positive="1")

F1_svc <- (2 * precision * recall) / (precision + recall)
```

##### As we can notice on above charts that too large and too small amount of degree can give not satisfying results

<br/><br/>

#### eXtreme Gradient Boosting (XGB, xgboost)

```{r echo=FALSE}
man_grid <- expand.grid(eta = 0.03,
                        max_depth = c(6, 7, 8, 9, 10, 11),
                        gamma = c(2, 3.5, 7, 10, 15, 20),
                        colsample_bytree = 0.53,
                        min_child_weight = 0,
                        subsample = 0.54,
                        nrounds = 650)

fitControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 5,
                           search = "grid")

model_xgb <- train(form = factor(target) ~ acousticness + danceability + duration_in_s + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo,
                   data = d.train,
                   method = "xgbTree",
                   trControl = fitControl,
                   verbose = TRUE,
                   tuneGrid = man_grid)

plot(model_xgb)
plot(model_xgb, metric = "Accuracy", plotType = "level")

pred_target_xgb <- predict(model_xgb, newdata = d.test)

precision <- posPredValue(pred_target_xgb, factor(d.test$target, levels=c(1,0)), positive="1")
recall <- sensitivity(pred_target_xgb, factor(d.test$target, levels=c(1,0)), positive="1")

F1_xgb <- (2 * precision * recall) / (precision + recall)
```

##### As we can notice on above charts that too large and too small amount of max depth of trees can give not satisfying results
##### Also it is obvious to observe that the small values of Minimum Loss Reduction are giving most satisfying results from considering range

<br/><br/>

#### Gradient Boosting Machines (GBM)

```{r echo=FALSE}
man_grid <- expand.grid(n.trees = seq(from = 10, to = 300, by = 20),
                        interaction.depth = seq(from = 1, to = 10, length.out = 6),
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

fitControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 5,
                           search = "grid")

model_gbm <- train(form = factor(target) ~ acousticness + danceability + duration_in_s + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo, 
                   data = d.train,
                   method = "gbm", 
                   trControl = fitControl,
                   verbose = FALSE,
                   tuneGrid = man_grid)

ggplot(model_gbm, xlab = "Boosting Iterations")
ggplot(model_gbm, metric = "Accuracy", plotType = "level", xlab = "Boosting Iterations")

pred_target_gbm <- predict(model_gbm, newdata = d.test)

precision <- posPredValue(pred_target_gbm,factor(d.test$target, levels=c(1,0)), positive="1")
recall <- sensitivity(pred_target_gbm, factor(d.test$target, levels=c(1,0)), positive="1")

F1_gbm <- (2 * precision * recall) / (precision + recall)
```

<br/><br/>

#### Random Forest

```{r echo=FALSE}
man_grid <- expand.grid(min.node.size = seq(from = 3, to = 12, by = 2),
                        mtry = seq(from = 1, to = 10, by = 2),
                        splitrule = c('gini', 'extratrees', 'hellinger'))

fitControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 5,
                           search = "grid")

model_random_forest <- train(form = factor(target) ~ acousticness + danceability + duration_in_s + energy + instrumentalness + liveness + loudness + mode + speechiness + tempo,
                             data = d.train,
                             method = "ranger",
                             trControl = fitControl,
                             verbose = TRUE,
                             tuneGrid = man_grid)

ggplot(model_random_forest)
ggplot(model_random_forest, metric = "Accuracy", plotType = "level")

pred_target_random_forest <- predict(model_random_forest, newdata = d.test)

precision <- posPredValue(pred_target_random_forest, factor(d.test$target, levels=c(1,0)), positive="1")
recall <- sensitivity(pred_target_random_forest, factor(d.test$target, levels=c(1,0)), positive="1")

F1_random_forest <- (2 * precision * recall) / (precision + recall)
```


```{r include=FALSE}
roc.model_svc <- roc(d.test$target, unfactor(pred_target_svc))
roc.model_gbm <- roc(d.test$target, unfactor(pred_target_gbm))
roc.model_random_forest <- roc(d.test$target, unfactor(pred_target_random_forest))
roc.model_xgb <- roc(d.test$target, unfactor(pred_target_xgb))
```

```{r echo=FALSE}
colors <- c("orange", "blue", "green", "purple")
plot(roc.model_svc, col=colors[1], main="ROC for SVC, GBM, Random Forest and XGB")
lines(roc.model_gbm, col=colors[2])
lines(roc.model_random_forest, col=colors[3])
lines(roc.model_xgb, col=colors[4])

lbls <- c("SVC", "GBM", "Random Forest", "XGB")
legend(0.2, 0.3, lbls, cex = 0.7, fill = colors)
```

<br/>

```{r echo=FALSE, message = TRUE}
cat('F1 score for SVC -> ', F1_svc)
cat('F1 score for GBM -> ', F1_gbm)
cat('F1 score for Random Forest -> ', F1_random_forest)
cat('F1 score for XGB -> ', F1_xgb)
```

<br/><br/>

```{r echo=FALSE}
colors <- c("orange", "blue", "green", "purple", "red", "gray")
plot(roc.model_gbm, col=colors[1], main="ROC for all classifiers")
lines(roc.model_svc, col=colors[2])
lines(roc.model_random_forest, col=colors[3])
lines(roc.model_xgb, col=colors[4])
lines(roc.model_lr, col=colors[5])
lines(roc.model_trees, col=colors[6])

label_trees <- paste('Trees')
label_lr <- paste('Logistic Regression')

lbls <- c("GBM", "SVC", "Random Forest", "XGB", "LR", "Trees")
legend(0.2, 0.3, lbls, cex = 0.7, fill = colors)
```

##### The best results gave us Random Forest Classifier. Perhaps in case of wider and longer tests on tuning hyperparameters we could achieve better results for XGBoost and GBM

<br/><br/>

### Summary
#### As it is possible to notice some features from music are really decisive about choosing value of target
#### Results and performances of models show that it is possible to predict user preferences.
#### Models with tuned hyperparameters gave us better results, which might be crucial when precision is really important,
#### but it is worth to mention and it is obvious also that this operation is really time consuming and hardware requires.
#### For purposes of visualization it was done with random options and then it was shown with plotting results on charts and dependence of parameters on model results

<br/>

